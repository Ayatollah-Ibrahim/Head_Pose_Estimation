# Face Pose Estimation from 2D Images

## Overview

This repository contains a Python implementation for estimating the yaw, pitch, and roll of human faces in images using a 3D facial landmark detection approach. The project utilizes the AFLW2000-3D dataset, which provides a comprehensive set of images annotated with 3D facial landmarks. By leveraging these annotations, we aim to build a robust model that accurately predicts head poses from 2D images, even in challenging orientations.

### Idea Behind the Project

The ability to estimate head poses is essential for various applications, including human-computer interaction, virtual reality, and facial recognition. Traditional 2D face detection techniques often struggle with poses that deviate significantly from frontal views. This project tackles the challenge by using 3D facial landmarks, allowing the model to better understand the spatial orientation of a face. 

### Methodology

1. **Dataset Usage**: The AFLW2000-3D dataset is critical for training and evaluating our models. Each image in the dataset is annotated with 68 facial landmarks, which provide key information about the face's geometry and pose.

2. **Feature Extraction**: The model computes various geometric features from the landmarks, such as distances and angles between key points. These features serve as input for our machine learning algorithms, which predict the head pose.

3. **Model Training**: We use several regression models, including Random Forest and XGBoost, to predict yaw, pitch, and roll. Each model is trained on the extracted features and evaluated based on its accuracy in predicting head poses.

4. **Evaluation Metrics**: Model performance is assessed using metrics like Mean Squared Error (MSE) and RÂ² score, providing insight into the effectiveness of the predictions.

## Dataset Reference

### AFLW2000-3D

The AFLW2000-3D dataset was introduced by Zhu et al. in their paper titled "Face Alignment Across Large Poses: A 3D Solution." This dataset contains 2000 images that have been annotated with image-level 68-point 3D facial landmarks, making it a valuable resource for evaluating 3D facial landmark detection models. The head poses in this dataset are highly diverse, often presenting challenges for CNN-based face detectors.

- **Citation**:
  ```
  Zhu, X., Wang, Z., & Liu, Y. (2016). Face Alignment Across Large Poses: A 3D Solution. In European Conference on Computer Vision (ECCV).
  ```

- **Source**: [AFLW2000-3D on TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/aflw2k3d)

## Installation

To run the code in this repository, you need to install the following packages:

```bash
pip install mediapipe opencv-python scipy pandas xgboost scikit-learn matplotlib
```

Ensure that you have Python 3.6 or later installed.

## Usage

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Ayatollah-Ibrahim/Head_Pose_Estimation.git
   cd repository-name
   ```

2. **Run the code**:
   You can execute the main script using:
   ```bash
   python face_pose_estimation.py
   ```

3. **Input Images**:
   Place your images in the specified JPEG folder. Modify the paths in the code if necessary.

## Code Structure

- `face_pose_estimation.py`: Main script for face pose estimation.
- `requirements.txt`: List of required packages.
- `data/`: Directory containing the AFLW2000-3D dataset and processed outputs.
- `README.md`: This documentation file.

## Detailed Explanation of Key Components

### 1. Dataset Creation

The `create_dataset` function is responsible for loading images and their corresponding landmark annotations. Each image is processed to extract the 68 facial landmarks, which serve as the foundation for further analysis.

### 2. Feature Extraction

We compute various geometric features based on the distances and angles between selected facial landmarks. For example, the distance between the eyes and the tip of the nose can be critical for determining yaw, while the position of the chin can help with pitch estimation.

### 3. Model Training

The implementation utilizes regression models like Random Forest and XGBoost. Each model is trained using the features extracted from the landmarks. Hyperparameters are tuned to optimize performance.

### 4. Evaluation

After training, we evaluate the models using a validation dataset. Metrics like Mean Squared Error (MSE) help us understand the accuracy of our predictions.

## Results

The results from model training and evaluation will be displayed in the terminal. You can visualize the predictions overlaid on images to assess the performance qualitatively.

### Example Output

Below are examples of the output images with predicted head poses:

![result1](https://github.com/user-attachments/assets/0a17406e-52ae-43ba-a33a-178bf70e53e0)
![result2](https://github.com/user-attachments/assets/e2c3a5ec-f224-4c15-b154-c8ac15d42190)


## Contributing

Contributions are welcome! Please open an issue or submit a pull request for any improvements, bug fixes, or additional features.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

